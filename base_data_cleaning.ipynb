{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a function for cleaning data\n",
    "def clean_data(data, acct_cols):\n",
    "    cleaned_data = data.copy()\n",
    "    \n",
    "    # Remove any leading or trailing whitespace in the account number column\n",
    "    for col in acct_cols:\n",
    "        cleaned_data[col] = cleaned_data[col].astype(str).str.strip()\n",
    "\n",
    "    # Check if any of the account number columns have invalid lengths\n",
    "    for col in acct_cols:\n",
    "        if cleaned_data[col].str.replace(\" \", \"\").str.len().ne(9).any():\n",
    "            print(f\"Warning: Account numbers in column '{col}' have invalid lengths and will be removed:\")\n",
    "            invalid_acct_nums = cleaned_data[col][cleaned_data[col].str.replace(\" \", \"\").str.len().ne(9)]\n",
    "            print(invalid_acct_nums.to_string(index=False))\n",
    "            cleaned_data.drop(index=invalid_acct_nums.index, inplace=True)\n",
    "\n",
    "    # Remove duplicates from the account number columns, if they exist\n",
    "    if len(acct_cols) > 0:\n",
    "        cleaned_data.drop_duplicates(subset=acct_cols, inplace=True)\n",
    "\n",
    "        # Check if there are any duplicate account numbers after removing duplicates\n",
    "        acct_counts = cleaned_data[acct_cols].value_counts()\n",
    "        duplicates = acct_counts[acct_counts > 1].reset_index()[acct_cols]\n",
    "        if len(duplicates) > 0:\n",
    "            print(\"Warning: Duplicate account numbers found in the following rows and will be removed:\")\n",
    "            print(duplicates.to_string(index=False))\n",
    "            cleaned_data.drop_duplicates(subset=acct_cols, keep=False, inplace=True)\n",
    "    \n",
    "    # Reset the index after cleaning\n",
    "    cleaned_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "# Common account number column names\n",
    "acct_col_names = [\"ACCT#\", \"ac_no\", \"Account_Number\", \"ACCOUNT_NUMBER\", \"ACCT_NO\", \"ACC_NO\"]\n",
    "\n",
    "# Ask the user to input the path to the folder containing the CSV or XLSX files they want to clean\n",
    "search_path = input(\"Enter the path to search for CSV or XLSX files: \")\n",
    "exts = (\".csv\", \".xlsx\")\n",
    "filenames = []\n",
    "for root, dirs, files in os.walk(search_path):\n",
    "    if os.path.basename(root) == \"cleaned\":\n",
    "        continue\n",
    "    for file in files:\n",
    "        if file.endswith(exts):\n",
    "            filenames.append(os.path.join(root, file))\n",
    "\n",
    "# Create a \"cleaned\" folder if it doesn't exist\n",
    "cleaned_folder = os.path.join(search_path, \"cleaned\")\n",
    "if not os.path.exists(cleaned_folder):\n",
    "    os.makedirs(cleaned_folder)\n",
    "\n",
    "# Clean each file individually\n",
    "for filename in filenames:\n",
    "    # Read the file into a Pandas DataFrame\n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename)\n",
    "    elif filename.endswith(\".xlsx\"):\n",
    "        data = pd.read_excel(filename)\n",
    "    else:\n",
    "        print(f\"Invalid file type. Skipping {filename}\")\n",
    "        continue\n",
    "    \n",
    "    # Check if any of the common account number column names are present\n",
    "    acct_cols = []\n",
    "    for col in data.columns:\n",
    "        if col in acct_col_names:\n",
    "            acct_cols.append(col)\n",
    "    \n",
    "    # Clean the data using the function we defined earlier, if an account number column is found\n",
    "    if len(acct_cols) > 0:\n",
    "        cleaned_data = clean_data(data, acct_cols)\n",
    "    else:\n",
    "        print(f\"No account number column found in {filename}\")\n",
    "        cleaned_data = data\n",
    "    \n",
    "    # Normalize the file name and add date and time to the cleaned file name\n",
    "    normalized_name = re.sub(r'\\s+', '_', os.path.splitext(os.path.basename(filename))[0]).lower()\n",
    "    cleaned_filename = f\"{normalized_name}_cleaned_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    cleaned_filename = os.path.join(cleaned_folder, cleaned_filename)\n",
    "    \n",
    "    # Save the cleaned data to a new file\n",
    "    cleaned_data.to_csv(cleaned_filename, index=False)\n",
    "    print(f\"{filename} cleaned and saved to {cleaned_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\cleaned_filename.csv cleaned and saved to C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\cleaned\\cleaned_filename_cleaned_20230311_023123.csv\n",
      "C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\QR Code project - Adenta 20230209.xlsx cleaned and saved to C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\cleaned\\qr_code_project_-_adenta_20230209_cleaned_20230311_023141.csv\n",
      "C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\QR Code project - Legon 20230209.xlsx cleaned and saved to C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\cleaned\\qr_code_project_-_legon_20230209_cleaned_20230311_023206.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a function for cleaning data\n",
    "def clean_data(data, acct_cols, cust_id_cols):\n",
    "    cleaned_data = data.copy()\n",
    "    \n",
    "    # Remove any leading or trailing whitespace in the account number column\n",
    "    for col in acct_cols:\n",
    "        cleaned_data[col] = cleaned_data[col].astype(str).str.strip()\n",
    "\n",
    "        # Check if any of the account number columns have invalid lengths\n",
    "        if cleaned_data[col].str.replace(\" \", \"\").str.len().ne(9).any():\n",
    "            print(f\"Warning: Account numbers in column '{col}' have invalid lengths and will be removed:\")\n",
    "            invalid_acct_nums = cleaned_data[col][cleaned_data[col].str.replace(\" \", \"\").str.len().ne(9)]\n",
    "            print(invalid_acct_nums.to_string(index=False))\n",
    "            cleaned_data.drop(index=invalid_acct_nums.index, inplace=True)\n",
    "\n",
    "    # Remove duplicates from the account number columns, if they exist\n",
    "    if len(acct_cols) > 0:\n",
    "        cleaned_data.drop_duplicates(subset=acct_cols, inplace=True)\n",
    "\n",
    "        # Check if there are any duplicate account numbers after removing duplicates\n",
    "        acct_counts = cleaned_data[acct_cols].value_counts()\n",
    "        duplicates = acct_counts[acct_counts > 1].reset_index()[acct_cols]\n",
    "        if len(duplicates) > 0:\n",
    "            print(\"Warning: Duplicate account numbers found in the following rows and will be removed:\")\n",
    "            print(duplicates.to_string(index=False))\n",
    "            cleaned_data.drop_duplicates(subset=acct_cols, keep=False, inplace=True)\n",
    "\n",
    "    # Clean the customer id column\n",
    "    for col in cust_id_cols:\n",
    "        if cleaned_data[col].str.contains(\"-\").any():\n",
    "            cleaned_data[col] = cleaned_data[col].str.split(\"-\").str[-1].str.strip().str.replace(\" \", \"\")\n",
    "    \n",
    "    # Reset the index after cleaning\n",
    "    cleaned_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "# Common account number column names\n",
    "acct_col_names = [\"ACCT#\", \"ac_no\", \"Account_Number\", \"ACCOUNT_NUMBER\", \"ACCT_NO\", \"ACC_NO\"]\n",
    "cust_id_col_names = [\"customer_id\", \"customerid\", \"customer_id_no\", \"CUSTOMER_ID\", \"customerid\", \"CustomerId\", \"CUST_ID\"]\n",
    "\n",
    "# Ask the user to input the path to the folder containing the CSV or XLSX files they want to clean\n",
    "search_path = input(\"Enter the path to search for CSV or XLSX files: \")\n",
    "exts = (\".csv\", \".xlsx\")\n",
    "filenames = []\n",
    "for root, dirs, files in os.walk(search_path):\n",
    "    if os.path.basename(root) == \"cleaned\":\n",
    "        continue\n",
    "    for file in files:\n",
    "        if file.endswith(exts):\n",
    "            filenames.append(os.path.join(root, file))\n",
    "\n",
    "# Create a \"cleaned\" folder if it doesn't exist\n",
    "cleaned_folder = os.path.join(search_path, \"cleaned\")\n",
    "if not os.path.exists(cleaned_folder):\n",
    "    os.makedirs(cleaned_folder)\n",
    "\n",
    "# Clean each file individually\n",
    "for filename in filenames:\n",
    "    # Read the file into a Pandas DataFrame\n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename)\n",
    "    elif filename.endswith(\".xlsx\"):\n",
    "        data = pd.read_excel(filename)\n",
    "    else:\n",
    "        print(f\"Invalid file type. Skipping {filename}\")\n",
    "        continue\n",
    "    \n",
    "    # Check if any of the common account number or customer id column names are present\n",
    "    acct_cols = []\n",
    "    cust_id_cols = []\n",
    "    for col in data.columns:\n",
    "        if col in acct_col_names:\n",
    "            acct_cols.append(col)\n",
    "        elif \"customer_id\" in col.lower():\n",
    "            cust_id_cols.append(col)\n",
    "\n",
    "    # Clean the data using the function we defined earlier, if an account number column is found\n",
    "    if len(acct_cols) > 0:\n",
    "        cleaned_data = clean_data(data, acct_cols, cust_id_cols)\n",
    "    else:\n",
    "        print(f\"No account number column found in {filename}\")\n",
    "        cleaned_data = data\n",
    "\n",
    "    # Clean the customer id column if it exists\n",
    "    if len(cust_id_cols) > 0:\n",
    "        for col in cust_id_cols:\n",
    "            cleaned_data[col] = cleaned_data[col].str.split(\"-\").str[-1].str.replace(\" \", \"\").str.strip()\n",
    "\n",
    "    # Normalize the file name and add date and time to the cleaned file name\n",
    "    normalized_name = re.sub(r'\\s+', '_', os.path.splitext(os.path.basename(filename))[0]).lower()\n",
    "    cleaned_filename = f\"{normalized_name}_cleaned_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    cleaned_filename = os.path.join(cleaned_folder, cleaned_filename)\n",
    "\n",
    "    # Save the cleaned data to a new file\n",
    "    cleaned_data.to_csv(cleaned_filename, index=False)\n",
    "    print(f\"{filename} cleaned and saved to {cleaned_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, cols):\n",
    "    cleaned_data = data.copy()\n",
    "\n",
    "    # Remove any leading or trailing whitespace in the columns\n",
    "    for col in cols:\n",
    "        cleaned_data[col] = cleaned_data[col].astype(str).str.strip()\n",
    "\n",
    "        # Check if any of the columns have invalid lengths\n",
    "        if col.startswith(\"ACC_NO\") and cleaned_data[col].str.replace(\" \", \"\").str.len().ne(9).any():\n",
    "            print(f\"Warning: Account numbers in column '{col}' have invalid lengths and will be removed:\")\n",
    "            invalid_acct_nums = cleaned_data[col][cleaned_data[col].str.replace(\" \", \"\").str.len().ne(9)]\n",
    "            print(invalid_acct_nums.to_string(index=False))\n",
    "            cleaned_data.drop(index=invalid_acct_nums.index, inplace=True)\n",
    "\n",
    "    # Remove duplicates from the columns, if they exist\n",
    "    if len(cols) > 0:\n",
    "        cleaned_data.drop_duplicates(subset=cols, inplace=True)\n",
    "\n",
    "        # Check if there are any duplicate values after removing duplicates\n",
    "        value_counts = cleaned_data[cols].apply(lambda x: tuple(x), axis=1).value_counts()\n",
    "        duplicates = value_counts[value_counts > 1].reset_index().rename(columns={\"index\": \"values\"})\n",
    "        if len(duplicates) > 0:\n",
    "            print(\"Warning: Duplicate values found in the following rows and will be removed:\")\n",
    "            print(duplicates.to_string(index=False))\n",
    "            cleaned_data.drop_duplicates(subset=cols, keep=False, inplace=True)\n",
    "\n",
    "    # Clean the columns\n",
    "    for col in cols:\n",
    "        if col.startswith(\"CUST_ID\") and cleaned_data[col].str.contains(\"-\").any():\n",
    "            cleaned_data[col] = cleaned_data[col].str.split(\"-\", n=1).str[1].str.strip()\n",
    "            cleaned_data[col] = cleaned_data[col].str.replace(r\"[^0-9]+\", \"\", regex=True)\n",
    "            has_letters = cleaned_data[col].str.contains(r\"[a-zA-Z]+\")\n",
    "            cleaned_data.loc[has_letters, col] = cleaned_data[col][has_letters]\n",
    "        elif col.startswith(\"TEL\"):\n",
    "            cleaned_data[col] = cleaned_data[col].astype(str).str.replace(\"[^0-9]\", \"\", regex=True)\n",
    "            cleaned_data[col] = cleaned_data[col].apply(lambda x: \"\" if len(x) < 9 or x.count(\"0\") > 3 else x)\n",
    "\n",
    "    # Reset the index after cleaning\n",
    "    cleaned_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "data = pd.read_excel(\"QR Code project - Adenta 20230209.xlsx\")\n",
    "cols = [\"ACC_NO\", \"CUST_ID\", \"TEL\"]\n",
    "cleaned_data = clean_data(data, cols)\n",
    "cleaned_data.to_csv(\"cleaned_filenamee.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "clean_data() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\base_data_cleaning.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Moshood/OneDrive/dev/python_data_profiling_scripts/base_data_cleaning/base_data_cleaning.ipynb#X20sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39m# Clean the data using the function we defined earlier, if an account number or customer id or phone number column is found\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Moshood/OneDrive/dev/python_data_profiling_scripts/base_data_cleaning/base_data_cleaning.ipynb#X20sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(acct_cols) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(cust_id_cols) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(phone_cols) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Moshood/OneDrive/dev/python_data_profiling_scripts/base_data_cleaning/base_data_cleaning.ipynb#X20sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     cleaned_data \u001b[39m=\u001b[39m clean_data(data, acct_cols, cust_id_cols, phone_cols)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Moshood/OneDrive/dev/python_data_profiling_scripts/base_data_cleaning/base_data_cleaning.ipynb#X20sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Moshood/OneDrive/dev/python_data_profiling_scripts/base_data_cleaning/base_data_cleaning.ipynb#X20sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo account number, customer id or phone number column found in \u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: clean_data() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "def clean_data(data, cols):\n",
    "    cleaned_data = data.copy()\n",
    "\n",
    "    # Remove any leading or trailing whitespace in the columns\n",
    "    for col in cols:\n",
    "        cleaned_data[col] = cleaned_data[col].astype(str).str.strip()\n",
    "\n",
    "        # Check if any of the columns have invalid lengths\n",
    "        if col.startswith(\"ACC_NO\") and cleaned_data[col].str.replace(\" \", \"\").str.len().ne(9).any():\n",
    "            print(f\"Warning: Account numbers in column '{col}' have invalid lengths and will be removed:\")\n",
    "            invalid_acct_nums = cleaned_data[col][cleaned_data[col].str.replace(\" \", \"\").str.len().ne(9)]\n",
    "            print(invalid_acct_nums.to_string(index=False))\n",
    "            cleaned_data.drop(index=invalid_acct_nums.index, inplace=True)\n",
    "\n",
    "    # Remove duplicates from the columns, if they exist\n",
    "    if len(cols) > 0:\n",
    "        cleaned_data.drop_duplicates(subset=cols, inplace=True)\n",
    "\n",
    "        # Check if there are any duplicate values after removing duplicates\n",
    "        value_counts = cleaned_data[cols].apply(lambda x: tuple(x), axis=1).value_counts()\n",
    "        duplicates = value_counts[value_counts > 1].reset_index().rename(columns={\"index\": \"values\"})\n",
    "        if len(duplicates) > 0:\n",
    "            print(\"Warning: Duplicate values found in the following rows and will be removed:\")\n",
    "            print(duplicates.to_string(index=False))\n",
    "            cleaned_data.drop_duplicates(subset=cols, keep=False, inplace=True)\n",
    "\n",
    "    # Clean the columns\n",
    "    for col in cols:\n",
    "        if col.startswith(\"CUST_ID\") and cleaned_data[col].str.contains(\"-\").any():\n",
    "            cleaned_data[col] = cleaned_data[col].str.split(\"-\", n=1).str[1].str.strip()\n",
    "            cleaned_data[col] = cleaned_data[col].str.replace(r\"[^0-9]+\", \"\", regex=True)\n",
    "            has_letters = cleaned_data[col].str.contains(r\"[a-zA-Z]+\")\n",
    "            cleaned_data.loc[has_letters, col] = cleaned_data[col][has_letters]\n",
    "        elif col.startswith(\"TEL\"):\n",
    "            cleaned_data[col] = cleaned_data[col].astype(str).str.replace(\"[^0-9]\", \"\", regex=True)\n",
    "            cleaned_data[col] = cleaned_data[col].apply(lambda x: \"\" if len(x) < 9 or x.count(\"0\") > 3 else x)\n",
    "\n",
    "    # Reset the index after cleaning\n",
    "    cleaned_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Ask the user to input the path to the folder containing the CSV or XLSX files they want to clean\n",
    "search_path = input(\"Enter the path to search for CSV or XLSX files: \")\n",
    "exts = (\".csv\", \".xlsx\")\n",
    "filenames = []\n",
    "for root, dirs, files in os.walk(search_path):\n",
    "    if os.path.basename(root) == \"cleaned\":\n",
    "        continue\n",
    "    for file in files:\n",
    "        if file.endswith(exts):\n",
    "            filenames.append(os.path.join(root, file))\n",
    "\n",
    "# Create a \"cleaned\" folder if it doesn't exist\n",
    "cleaned_folder = os.path.join(search_path, \"cleaned\")\n",
    "if not os.path.exists(cleaned_folder):\n",
    "    os.makedirs(cleaned_folder)\n",
    "\n",
    "# Clean each file individually\n",
    "for filename in filenames:\n",
    "    # Read the file into a Pandas DataFrame\n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename)\n",
    "    elif filename.endswith(\".xlsx\"):\n",
    "        data = pd.read_excel(filename)\n",
    "    else:\n",
    "        print(f\"Invalid file type. Skipping {filename}\")\n",
    "        continue\n",
    "    \n",
    "    # Check if any of the common account number or customer id column names are present\n",
    "    acct_cols = []\n",
    "    cust_id_cols = []\n",
    "    phone_cols = []\n",
    "    for col in data.columns:\n",
    "        if col.upper() in acct_col_names:\n",
    "            acct_cols.append(col)\n",
    "        elif \"CUSTOMER\" in col.upper() and \"ID\" in col.upper():\n",
    "            cust_id_cols.append(col)\n",
    "        elif \"PHONE\" in col.upper() or \"TELEPHONE\" in col.upper():\n",
    "            phone_cols.append(col)\n",
    "\n",
    "    # Clean the data using the function we defined earlier, if an account number or customer id or phone number column is found\n",
    "    if len(acct_cols) > 0 or len(cust_id_cols) > 0 or len(phone_cols) > 0:\n",
    "        cleaned_data = clean_data(data, acct_cols, cust_id_cols, phone_cols)\n",
    "    else:\n",
    "        print(f\"No account number, customer id or phone number column found in {filename}\")\n",
    "        cleaned_data = data\n",
    "\n",
    "    # Normalize the file name and add date and time to the cleaned file name\n",
    "    normalized_name = re.sub(r'\\s+', '_', os.path.splitext(os.path.basename(filename))[0]).lower()\n",
    "    cleaned_filename = f\"{normalized_name}_cleaned_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    cleaned_filename = os.path.join(cleaned_folder, cleaned_filename)\n",
    "\n",
    "    # Save the cleaned data to a new file\n",
    "    cleaned_data.to_csv(cleaned_filename, index=False)\n",
    "    print(f\"{filename} cleaned and saved to {cleaned_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report for QR Code project - Adenta 20230209.xlsx:\n",
      "Number of invalid account number lengths removed: 0\n",
      "Number of duplicate account numbers removed: 0\n",
      "Number of customer ID values changed: 0\n",
      "Number of phone number values changed: 33698\n",
      "C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\QR Code project - Adenta 20230209.xlsx cleaned and saved to C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\cleaned\\qr_code_project_-_adenta_20230209_cleaned_20230311_101703.csv\n",
      "\n",
      "Report for QR Code project - Legon 20230209.xlsx:\n",
      "Number of invalid account number lengths removed: 0\n",
      "Number of duplicate account numbers removed: 0\n",
      "Number of customer ID values changed: 0\n",
      "Number of phone number values changed: 60424\n",
      "C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\QR Code project - Legon 20230209.xlsx cleaned and saved to C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\cleaned\\qr_code_project_-_legon_20230209_cleaned_20230311_101726.csv\n"
     ]
    }
   ],
   "source": [
    "def clean_data(data, acct_cols, cust_id_cols, phone_cols):\n",
    "    cleaned_data = data.copy()\n",
    "\n",
    "    # Define variables to track counts of changes made\n",
    "    acct_invalid_lengths = 0\n",
    "    acct_duplicates_removed = 0\n",
    "    cust_id_values_changed = 0\n",
    "    phone_no_values_changed = 0\n",
    "\n",
    "    # Remove any leading or trailing whitespace in the account number column\n",
    "    for col in acct_cols:\n",
    "        cleaned_data[col] = cleaned_data[col].astype(str).str.strip()\n",
    "\n",
    "        # Check if any of the account number columns have invalid lengths\n",
    "        if cleaned_data[col].str.replace(\" \", \"\").str.len().ne(9).any():\n",
    "            print(f\"Warning: Account numbers in column '{col}' have invalid lengths and will be removed:\")\n",
    "            invalid_acct_nums = cleaned_data[col][cleaned_data[col].str.replace(\" \", \"\").str.len().ne(9)]\n",
    "            print(invalid_acct_nums.to_string(index=False))\n",
    "            cleaned_data.drop(index=invalid_acct_nums.index, inplace=True)\n",
    "            acct_invalid_lengths += len(invalid_acct_nums)\n",
    "\n",
    "    # Remove duplicates from the account number columns, if they exist\n",
    "    if len(acct_cols) > 0:\n",
    "        cleaned_data.drop_duplicates(subset=acct_cols, inplace=True)\n",
    "\n",
    "        # Check if there are any duplicate account numbers after removing duplicates\n",
    "        acct_counts = cleaned_data[acct_cols].value_counts()\n",
    "        duplicates = acct_counts[acct_counts > 1].reset_index()[acct_cols]\n",
    "        if len(duplicates) > 0:\n",
    "            print(\"Warning: Duplicate account numbers found in the following rows and will be removed:\")\n",
    "            print(duplicates.to_string(index=False))\n",
    "            cleaned_data.drop_duplicates(subset=acct_cols, keep=False, inplace=True)\n",
    "            acct_duplicates_removed += len(duplicates)\n",
    "\n",
    "    # Clean the customer id column\n",
    "    for col in cust_id_cols:\n",
    "        if cleaned_data[col].str.contains(\"-\").any():\n",
    "            cleaned_data[col] = cleaned_data[col].str.split(\"-\", n=1).str[1].str.strip()\n",
    "            cleaned_data[col] = cleaned_data[col].str.replace(r\"[^0-9]+\", \"\", regex=True)\n",
    "            has_letters = cleaned_data[col].str.contains(r\"[a-zA-Z]+\")\n",
    "            cleaned_data.loc[has_letters, col] = cleaned_data[col][has_letters]\n",
    "            cust_id_values_changed += sum(has_letters)\n",
    "\n",
    "    # Clean the phone number column\n",
    "    for col in phone_cols:\n",
    "        cleaned_data[col] = cleaned_data[col].astype(str).str.replace(\"[^0-9]\", \"\", regex=True)\n",
    "        cleaned_data[col] = cleaned_data[col].apply(lambda x: \"\" if len(x) < 9 or x.count(\"0\") > 3 or x.count(\"00\") > 0 else x)\n",
    "        phone_no_values_changed += len(cleaned_data[col]) - cleaned_data[col].astype(bool).sum(axis=0)\n",
    "\n",
    "    # Reset the index after cleaning\n",
    "    cleaned_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Print the report\n",
    "    print(f\"\\nReport for {os.path.basename(filename)}:\")\n",
    "    print(f\"Number of invalid account number lengths removed: {acct_invalid_lengths}\")\n",
    "    print(f\"Number of duplicate account numbers removed: {acct_duplicates_removed}\")\n",
    "    print(f\"Number of customer ID values changed: {cust_id_values_changed}\")\n",
    "    print(f\"Number of phone number values changed: {phone_no_values_changed}\")\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "acct_col_names = [\"ACC_NO\"]\n",
    "cust_id_col_names = [\"CUST_ID\"]\n",
    "phone_no_col_names = [\"TEL\"]\n",
    "\n",
    "# Ask the user to input the path to the folder containing the CSV or XLSX files they want to clean\n",
    "search_path = input(\"Enter the path to search for CSV or XLSX files: \")\n",
    "exts = (\".csv\", \".xlsx\")\n",
    "filenames = []\n",
    "for root, dirs, files in os.walk(search_path):\n",
    "    if os.path.basename(root) == \"cleaned\":\n",
    "        continue\n",
    "    for file in files:\n",
    "        if file.endswith(exts):\n",
    "            filenames.append(os.path.join(root, file))\n",
    "\n",
    "# Create a \"cleaned\" folder if it doesn't exist\n",
    "cleaned_folder = os.path.join(search_path, \"cleaned\")\n",
    "if not os.path.exists(cleaned_folder):\n",
    "    os.makedirs(cleaned_folder)\n",
    "\n",
    "# Clean each file individually\n",
    "for filename in filenames:\n",
    "    # Read the file into a Pandas DataFrame\n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename)\n",
    "    elif filename.endswith(\".xlsx\"):\n",
    "        data = pd.read_excel(filename)\n",
    "    else:\n",
    "        print(f\"Invalid file type. Skipping {filename}\")\n",
    "        continue\n",
    "    \n",
    "    # Check if any of the common account number or customer id column names are present\n",
    "    acct_cols = []\n",
    "    cust_id_cols = []\n",
    "    phone_cols = []\n",
    "    for col in data.columns:\n",
    "        if col in acct_col_names:\n",
    "            acct_cols.append(col)\n",
    "        elif col in cust_id_col_names:\n",
    "            cust_id_cols.append(col)\n",
    "        elif col in phone_no_col_names:\n",
    "            phone_cols.append(col)\n",
    "\n",
    "    # Clean the data using the function we defined earlier, if an account number or customer id or phone number column is found\n",
    "    if len(acct_cols) > 0 or len(cust_id_cols) > 0 or len(phone_cols) > 0:\n",
    "        cleaned_data = clean_data(data, acct_cols, cust_id_cols, phone_cols)\n",
    "    else:\n",
    "        print(f\"No account number, customer id or phone number column found in {filename}\")\n",
    "        cleaned_data = data\n",
    "\n",
    "    # Normalize the file name and add date and time to the cleaned file name\n",
    "    normalized_name = re.sub(r'\\s+', '_', os.path.splitext(os.path.basename(filename))[0]).lower()\n",
    "    cleaned_filename = f\"{normalized_name}_cleaned_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    cleaned_filename = os.path.join(cleaned_folder, cleaned_filename)\n",
    "\n",
    "    # Save the cleaned data to a new file\n",
    "    cleaned_data.to_csv(cleaned_filename, index=False)\n",
    "    print(f\"{filename} cleaned and saved to {cleaned_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 96102 rows in 3 columns\n",
      "Number of invalid account numbers removed: 0\n",
      "Number of duplicate rows removed: 0\n",
      "Number of phone numbers cleaned: 71047\n",
      "C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\QR Code project - Adenta 20230209.xlsx cleaned and saved to C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\cleaned\\qr_code_project_-_adenta_20230209_cleaned_20230311_174115.csv\n",
      "\n",
      "Cleaned data report:\n",
      "====================\n",
      "File name: C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\QR Code project - Adenta 20230209.xlsx\n",
      "Number of rows before cleaning: 96102\n",
      "Number of rows after cleaning: 96100\n",
      "Removed 2 rows with invalid account numbers.\n",
      "Processed 121466 rows in 3 columns\n",
      "Number of invalid account numbers removed: 0\n",
      "Number of duplicate rows removed: 0\n",
      "Number of phone numbers cleaned: 81431\n",
      "C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\QR Code project - Legon 20230209.xlsx cleaned and saved to C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\cleaned\\qr_code_project_-_legon_20230209_cleaned_20230311_174138.csv\n",
      "\n",
      "Cleaned data report:\n",
      "====================\n",
      "File name: C:\\Users\\Moshood\\OneDrive\\dev\\python_data_profiling_scripts\\base_data_cleaning\\QR Code project - Legon 20230209.xlsx\n",
      "Number of rows before cleaning: 121466\n",
      "Number of rows after cleaning: 121140\n",
      "Removed 326 rows with invalid account numbers.\n"
     ]
    }
   ],
   "source": [
    "def clean_data(data, acct_cols, cust_id_cols, phone_cols):\n",
    "    cleaned_data = data.copy()\n",
    "    \n",
    "    # Initialize counters\n",
    "    num_invalid_acct_nums = 0\n",
    "    num_duplicates_removed = 0\n",
    "    num_phone_numbers_cleaned = 0\n",
    "\n",
    "    # Remove any leading or trailing whitespace in the account number column\n",
    "    for col in acct_cols:\n",
    "        cleaned_data[col] = cleaned_data[col].astype(str).str.strip()\n",
    "\n",
    "        # Check if any of the account number columns have invalid lengths\n",
    "        if cleaned_data[col].str.replace(\" \", \"\").str.len().ne(9).any():\n",
    "            print(f\"Warning: Account numbers in column '{col}' have invalid lengths and will be removed:\")\n",
    "            invalid_acct_nums = cleaned_data[col][cleaned_data[col].str.replace(\" \", \"\").str.len().ne(9)]\n",
    "            print(invalid_acct_nums.to_string(index=False))\n",
    "            cleaned_data.drop(index=invalid_acct_nums.index, inplace=True)\n",
    "            num_invalid_acct_nums += len(invalid_acct_nums)\n",
    "\n",
    "    # Remove duplicates from the account number columns, if they exist\n",
    "    if len(acct_cols) > 0:\n",
    "        cleaned_data.drop_duplicates(subset=acct_cols, inplace=True)\n",
    "\n",
    "        # Check if there are any duplicate account numbers after removing duplicates\n",
    "        acct_counts = cleaned_data[acct_cols].value_counts()\n",
    "        duplicates = acct_counts[acct_counts > 1].reset_index()[acct_cols]\n",
    "        if len(duplicates) > 0:\n",
    "            print(\"Warning: Duplicate account numbers found in the following rows and will be removed:\")\n",
    "            print(duplicates.to_string(index=False))\n",
    "            cleaned_data.drop_duplicates(subset=acct_cols, keep=False, inplace=True)\n",
    "            num_duplicates_removed += len(duplicates)\n",
    "\n",
    "    # Clean the customer id column\n",
    "    for col in cust_id_cols:\n",
    "        if cleaned_data[col].str.contains(\"-\").any():\n",
    "            cleaned_data[col] = cleaned_data[col].str.split(\"-\", n=1).str[1].str.strip()\n",
    "            cleaned_data[col] = cleaned_data[col].str.replace(r\"[^0-9]+\", \"\", regex=True)\n",
    "            has_letters = cleaned_data[col].str.contains(r\"[a-zA-Z]+\")\n",
    "            cleaned_data.loc[has_letters, col] = cleaned_data[col][has_letters]\n",
    "\n",
    "    # Clean the phone number column\n",
    "    for col in phone_cols:\n",
    "        cleaned_data[col] = cleaned_data[col].astype(str).str.replace(\"[^0-9]\", \"\", regex=True)\n",
    "        num_phone_numbers_cleaned += (cleaned_data[col].str.len() >= 9).sum()\n",
    "        cleaned_data[col] = cleaned_data[col].apply(lambda x: \"\" if len(x) < 9 or x.count(\"0\") > 3 else x)\n",
    "    \n",
    "    # Reset the index after cleaning\n",
    "    cleaned_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Print report\n",
    "    print(f\"Processed {len(data)} rows in {len(acct_cols) + len(cust_id_cols) + len(phone_cols)} columns\")\n",
    "    print(f\"Number of invalid account numbers removed: {num_invalid_acct_nums}\")\n",
    "    print(f\"Number of duplicate rows removed: {num_duplicates_removed}\")\n",
    "    print(f\"Number of phone numbers cleaned: {num_phone_numbers_cleaned}\")\n",
    "    \n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "acct_col_names = [\"ACC_NO\"]\n",
    "cust_id_col_names = [\"CUST_ID\"]\n",
    "phone_no_col_names = [\"TEL\"]\n",
    "\n",
    "# Ask the user to input the path to the folder containing the CSV or XLSX files they want to clean\n",
    "search_path = input(\"Enter the path to search for CSV or XLSX files: \")\n",
    "exts = (\".csv\", \".xlsx\")\n",
    "filenames = []\n",
    "for root, dirs, files in os.walk(search_path):\n",
    "    if os.path.basename(root) == \"cleaned\":\n",
    "        continue\n",
    "    for file in files:\n",
    "        if file.endswith(exts):\n",
    "            filenames.append(os.path.join(root, file))\n",
    "\n",
    "# Create a \"cleaned\" folder if it doesn't exist\n",
    "cleaned_folder = os.path.join(search_path, \"cleaned\")\n",
    "if not os.path.exists(cleaned_folder):\n",
    "    os.makedirs(cleaned_folder)\n",
    "\n",
    "# Clean each file individually\n",
    "for filename in filenames:\n",
    "    # Read the file into a Pandas DataFrame\n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(filename)\n",
    "    elif filename.endswith(\".xlsx\"):\n",
    "        data = pd.read_excel(filename)\n",
    "    else:\n",
    "        print(f\"Invalid file type. Skipping {filename}\")\n",
    "        continue\n",
    "    \n",
    "    # Check if any of the common account number or customer id column names are present\n",
    "    acct_cols = []\n",
    "    cust_id_cols = []\n",
    "    phone_cols = []\n",
    "    for col in data.columns:\n",
    "        if col in acct_col_names:\n",
    "            acct_cols.append(col)\n",
    "        elif col in cust_id_col_names:\n",
    "            cust_id_cols.append(col)\n",
    "        elif col in phone_no_col_names:\n",
    "            phone_cols.append(col)\n",
    "    \n",
    "    if len(acct_cols) > 0 or len(cust_id_cols) > 0 or len(phone_cols) > 0:\n",
    "        cleaned_data = clean_data(data, acct_cols, cust_id_cols, phone_cols)\n",
    "    else:\n",
    "        print(f\"No account number, customer id or phone number column found in {filename}\")\n",
    "        cleaned_data = data\n",
    "\n",
    "    normalized_name = re.sub(r'\\s+', '_', os.path.splitext(os.path.basename(filename))[0]).lower()\n",
    "    cleaned_filename = f\"{normalized_name}_cleaned_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    cleaned_filename = os.path.join(cleaned_folder, cleaned_filename)\n",
    "\n",
    "    cleaned_data.to_csv(cleaned_filename, index=False)\n",
    "\n",
    "    print(f\"{filename} cleaned and saved to {cleaned_filename}\")\n",
    "\n",
    "       # Print report on changes made to the file\n",
    "    print(\"\\nCleaned data report:\")\n",
    "    print(\"====================\")\n",
    "    print(f\"File name: {filename}\")\n",
    "    print(f\"Number of rows before cleaning: {len(data)}\")\n",
    "    print(f\"Number of rows after cleaning: {len(cleaned_data)}\")\n",
    "\n",
    "    # Account number\n",
    "    if len(acct_cols) > 0:\n",
    "        removed_accounts = len(data) - len(cleaned_data)\n",
    "        if removed_accounts > 0:\n",
    "            print(f\"Removed {removed_accounts} rows with invalid account numbers.\")\n",
    "        acct_counts_before = data[acct_cols].nunique().values\n",
    "        acct_counts_after = cleaned_data[acct_cols].nunique().values\n",
    "        diff = acct_counts_before - acct_counts_after\n",
    "        if (diff > 0).any():\n",
    "            print(f\"Removed {diff.sum()} duplicate account numbers.\")\n",
    "            for i, col in enumerate(acct_cols):\n",
    "                if diff[i] > 0:\n",
    "                    print(f\"  {diff[i]} duplicates removed from column '{col}'.\")\n",
    "    else:\n",
    "        print(\"No account number column found.\")\n",
    "\n",
    "    # if len(cust_id_cols) > 0:\n",
    "    #     common_cols = list(set(cust_id_cols).intersection(set(data.columns)))\n",
    "    #     data_common, cleaned_common = data[common_cols], cleaned_data[common_cols]\n",
    "    #     data_common, cleaned_common = data_common.align(cleaned_common)\n",
    "    #     changed_cust_ids = (cleaned_common != data_common).any(axis=1).sum()\n",
    "    #     if changed_cust_ids > 0:\n",
    "    #         print(f\"Changed {changed_cust_ids} customer IDs.\")\n",
    "\n",
    "    \n",
    "    # # Phone number\n",
    "    # if len(phone_cols) > 0:\n",
    "    #     removed_phone_nums = (data[phone_cols].astype(str) != cleaned_data[phone_cols].astype(str)).any(axis=1).sum()\n",
    "    #     if removed_phone_nums > 0:\n",
    "    #         print(f\"Removed {removed_phone_nums} rows with invalid phone numbers.\")\n",
    "    # else:\n",
    "    #     print(\"No phone number column found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3e10ef16274dd72e574b8fa73b58450b957d8421a2901baded3cca26fcf5dda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
